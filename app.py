"""Streamlit interface for the Akiya-Lens residual model.

ã“ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯ ``docs/æ®‹å·®ãƒ¢ãƒ‡ãƒ«ã¨ã‚¢ãƒ—ãƒªè¨­è¨ˆä»•æ§˜.md`` ã®ä»•æ§˜ã«å¾“ã„ã¾ã™ã€‚
ãƒã‚¹ã‚¿ãƒ¼ç‰¹å¾´é‡ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ + æ®‹å·®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å†æ§‹ç¯‰ã—ã€
ãƒªã‚¹ã‚¯ã‚«ãƒ†ã‚´ãƒªã€äºˆæ¸¬ã€SHAPç”±æ¥ã®èª¬æ˜ã‚’å«ã‚€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªå¸‚ç”ºæ‘ãƒãƒƒãƒ—ã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚

"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

import numpy as np
import pandas as pd
import streamlit as st

try:
    import folium
    from folium import features
    from branca.colormap import linear as branca_linear
    from branca.element import MacroElement, Template
    from streamlit_folium import st_folium
except ImportError:  # pragma: no cover - handled gracefully in UI
    folium = None
    features = None
    branca_linear = None
    MacroElement = None
    Template = None
    st_folium = None

try:
    from catboost import CatBoostRegressor, Pool
except ImportError:  # pragma: no cover - handled gracefully in UI
    CatBoostRegressor = None  # type: ignore
    Pool = None  # type: ignore


# ---------------------------------------------------------------------------
# Paths & constants
# ---------------------------------------------------------------------------

REPO_ROOT = Path(__file__).resolve().parent
DATA_PATH = (
    REPO_ROOT / "data/processed/features_master__wide__v1.csv"
)  # å¸‚åŒºç”ºæ‘ãƒ‡ãƒ¼ã‚¿(ç‰¹å¾´é‡)
GEOJSON_PATH = REPO_ROOT / "data/geojson/municipalities.geojson"  # åœ°å›³ãƒ‡ãƒ¼ã‚¿(geojson)
MODEL_PATH = REPO_ROOT / "models/final_diff_model.cbm"  # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«(CatBoost)
METRICS_PATH = REPO_ROOT / "data/processed/model_metrics.json"  # è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹

DEFAULT_TOLERANCE = 0.1  # "æ¨ªã°ã„"ã¨ã™ã‚‹é–¾å€¤ (ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼UIã§èª¿æ•´å¯)
# å¸‚åŒºç”ºæ‘å¡—ã‚Šã¤ã¶ã—ã‚»ãƒ¬ã‚¯ãƒˆ
MAP_OPTIONS = (
    "4æ®µéšãƒªã‚¹ã‚¯",
    "2023å¹´ç©ºãå®¶ç‡ï¼ˆå®Ÿæ¸¬ï¼‰",
    "2023å¹´ç©ºãå®¶ç‡ï¼ˆäºˆæ¸¬ï¼‰",
    "æ®‹å·®ï¼ˆå®Ÿæ¸¬âˆ’äºˆæ¸¬ï¼‰",
)
# ãƒªã‚¹ã‚¯ãƒ©ãƒ™ãƒ«å®šç¾©ï¼ˆè‰²ãƒ»å‡¡ä¾‹ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€å…ƒç®¡ç†ï¼‰
RISK_LEVELS = {
    "(æœ€å„ªå…ˆ)": {"color": "#d73027", "legend": "èµ¤(æœ€å„ªå…ˆ) é«˜ãƒ»âš"},
    "(æ³¨æ„)": {"color": "#fc8d59", "legend": "ã‚ªãƒ¬ãƒ³ã‚¸(æ³¨æ„) é«˜ãƒ»æ¨ª/â†“"},
    "(è­¦æˆ’)": {"color": "#fee08b", "legend": "é»„(è­¦æˆ’) ä½ãƒ»âš"},
    "(ä½)": {"color": "#1a9850", "legend": "ç·‘(ä½) ä½ãƒ»æ¨ª/â†“"},
}

# é«˜ä½Ã—ãƒˆãƒ¬ãƒ³ãƒ‰ã®çµ„åˆã›ã”ã¨ã®ãƒ©ãƒ™ãƒ«å‰²ã‚Šå½“ã¦
RISK_RULES = {
    (True, "å¢—"): "(æœ€å„ªå…ˆ)",
    (True, "æ¨ªã°ã„"): "(æ³¨æ„)",
    (True, "æ¸›"): "(æ³¨æ„)",
    (False, "å¢—"): "(è­¦æˆ’)",
    (False, "æ¨ªã°ã„"): "(ä½)",
    (False, "æ¸›"): "(ä½)",
}

RISK_LEGEND_ORDER = ["(æœ€å„ªå…ˆ)", "(æ³¨æ„)", "(è­¦æˆ’)", "(ä½)"]
DEFAULT_RISK_LABEL = "(ä½)"

NAN_COLOR = "#d9d9d9"


# ---------------------------------------------------------------------------
# Data loading utilities
# ---------------------------------------------------------------------------


def _normalise_columns(df: pd.DataFrame) -> pd.DataFrame:
    df.columns = [c.lstrip("\ufeff") for c in df.columns]  # BOMé™¤å»
    return df


@st.cache_data(show_spinner=False)  # "å®Ÿè¡Œä¸­â€¦ã ã•ãªã„"
def load_features(path: Path) -> pd.DataFrame:
    if not path.exists():
        raise FileNotFoundError(f"ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
    df = pd.read_csv(path)
    df = _normalise_columns(df)
    df["å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰"] = (
        df["å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰"]
        .astype(str)
        .str.replace(r"\.0$", "", regex=True)
        .str.zfill(5)  # éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰1ã€œ9ã‚’0åŸ‹ã‚
    )
    return df


@st.cache_data(show_spinner=False)
def load_geojson(path: Path) -> Optional[dict[str, Any]]:
    if not path.exists():
        return None
    with path.open("r", encoding="utf-8") as handle:
        return json.load(handle)


@st.cache_data(show_spinner=False)
def load_model_metrics(path: Path) -> Dict[str, Any]:
    if not path.exists():
        return {}
    with path.open("r", encoding="utf-8") as handle:
        return json.load(handle)


@st.cache_resource(show_spinner=False)
def load_catboost_model(path: Path) -> Optional[CatBoostRegressor]:
    if CatBoostRegressor is None:
        return None
    if not path.exists():
        return None
    model = CatBoostRegressor()
    model.load_model(str(path))
    return model


# ---------------------------------------------------------------------------
# Feature utilities
# ---------------------------------------------------------------------------


def _get_numeric_series(df: pd.DataFrame, column: str) -> pd.Series:
    if column not in df.columns:
        return pd.Series(np.nan, index=df.index, dtype="float64")

    series = df[column]
    if pd.api.types.is_numeric_dtype(series):
        return pd.to_numeric(series, errors="coerce")

    cleaned = (
        series.astype(str)
        .str.replace(",", "", regex=False)
        .str.replace("%", "", regex=False)
        .str.replace("â€°", "", regex=False)
    )
    return pd.to_numeric(cleaned, errors="coerce")


# ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´é‡ã‚’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ã—ã¦ã—ã¦æŠ½å‡º
def build_model_feature_matrix(
    df: pd.DataFrame, feature_names: Iterable[str]
) -> Tuple[pd.DataFrame, List[str]]:
    feature_list = list(feature_names)
    matrix = pd.DataFrame(index=df.index)
    missing: List[str] = []  # å…ƒãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ãªã‹ã£ãŸã‚‰åˆ—åã‚’æ ¼ç´

    for name in feature_list:
        if name in df.columns:
            series = df[name]
            if pd.api.types.is_numeric_dtype(series):
                matrix[name] = series
            else:
                # å¿µã®ç‚ºãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
                numeric = (
                    series.astype(str)
                    .str.replace(",", "", regex=False)
                    .str.replace("%", "", regex=False)
                    .str.replace("â€°", "", regex=False)
                )
                numeric = pd.to_numeric(numeric, errors="coerce")
                # åŠåˆ†ãã‚‰ã„æ•°å€¤å¤‰æ›ã§ãã‚‹åˆ—ã¯ä½¿ãˆã‚‹ã‚‚ã®ã¨ã™ã‚‹
                if numeric.notna().sum() >= series.notna().sum() * 0.5:
                    matrix[name] = numeric
                else:
                    matrix[name] = series
        else:
            missing.append(name)
            matrix[name] = pd.Series(np.nan, index=df.index)

    return matrix[feature_list], missing


def enrich_diff_model_features(df: pd.DataFrame) -> pd.DataFrame:
    """å·®åˆ†ãƒ¢ãƒ‡ãƒ«ç”¨ã®å‰å‡¦ç†ã‚’é©ç”¨ã—ã€å­¦ç¿’æ™‚ã®ç‰¹å¾´é‡ã‚’å†ç”Ÿæˆã™ã‚‹ã€‚"""

    augmented = df.copy()

    diff_specs = [
        ("ä½å®…åœ°ä¾¡_logä¸­å¤®å€¤_å¤‰åŒ–é‡", "ä½å®…åœ°ä¾¡_logä¸­å¤®å€¤_2023", "ä½å®…åœ°ä¾¡_logä¸­å¤®å€¤_2018"),
        ("Î”å‡ºç”Ÿç‡", "2023_å‡ºç”Ÿç‡[â€°]", "2018_å‡ºç”Ÿç‡[â€°]"),
        ("Î”æ­»äº¡ç‡", "2023_æ­»äº¡ç‡[â€°]", "2018_æ­»äº¡ç‡[â€°]"),
        ("Î”å¹´å°‘äººå£ç‡", "2023_å¹´å°‘äººå£ç‡[%]", "2018_å¹´å°‘äººå£ç‡[%]"),
        ("Î”é«˜é½¢åŒ–ç‡", "2023_é«˜é½¢åŒ–ç‡[%]", "2018_é«˜é½¢åŒ–ç‡[%]"),
        ("Î”ç”Ÿç”£å¹´é½¢äººå£ç‡", "2023_ç”Ÿç”£å¹´é½¢äººå£ç‡[%]", "2018_ç”Ÿç”£å¹´é½¢äººå£ç‡[%]"),
        ("Î”è»¢å…¥è¶…éç‡", "2023_è»¢å…¥è¶…éç‡[â€°]", "2018_è»¢å…¥è¶…éç‡[â€°]"),
    ]

    for new_col, col_recent, col_base in diff_specs:
        if col_recent in augmented.columns and col_base in augmented.columns:
            augmented[new_col] = _get_numeric_series(augmented, col_recent) - _get_numeric_series(
                augmented, col_base
            )

    if "éç–åœ°åŸŸå¸‚ç”ºæ‘" in augmented.columns:
        dummies = pd.get_dummies(
            augmented["éç–åœ°åŸŸå¸‚ç”ºæ‘"], prefix="éç–åœ°åŸŸå¸‚ç”ºæ‘"
        )
        for col in dummies.columns:
            augmented[col] = dummies[col]

    return augmented


# ---------------------------------------------------------------------------
# Prediction & SHAP utilities
# ---------------------------------------------------------------------------


def _fit_baseline(baseline: pd.Series, target: pd.Series) -> tuple[float, float]:
    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³(å˜å›å¸°å¼)ã§æ¨è«–
    valid_mask = baseline.notna() & target.notna()  # æ¬ æå€¤ã®ãªã„è¡Œã ã‘ä½¿ç”¨
    if valid_mask.sum() < 2:  # æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒ2ç‚¹ä»¥ä¸Šå–ã‚Œãªã‘ã‚Œã°0ã‚’ã£è¿”å´
        return 0.0, 0.0
    x = baseline.loc[valid_mask].to_numpy(dtype=float)
    y = target.loc[valid_mask].to_numpy(dtype=float)
    slope, intercept = np.polyfit(x, y, 1)
    return float(slope), float(intercept)  # å‚¾ãã¨åˆ‡ç‰‡ã‚’è¿”å´


def compute_predictions(
    df: pd.DataFrame,
    model: Optional[CatBoostRegressor],
) -> tuple[pd.DataFrame, Optional[pd.DataFrame], List[str]]:
    messages: List[str] = []

    vac18 = _get_numeric_series(df, "ç©ºãå®¶ç‡_2018")
    vac23 = _get_numeric_series(df, "ç©ºãå®¶ç‡_2023")
    delta_observed = vac23 - vac18  # æ®‹å·®

    slope, intercept = _fit_baseline(vac18, delta_observed)
    base_pred = vac18 * slope + intercept  # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã§äºˆæ¸¬å€¤ã‚’è¨ˆç®—

    feature_matrix: Optional[pd.DataFrame] = None
    resid_pred = pd.Series(0.0, index=df.index, dtype=float)

    if model is not None and hasattr(model, "feature_names_"):
        feature_names = list(model.feature_names_)
        if feature_names:
            augmented_df = enrich_diff_model_features(df)
            feature_matrix, missing = build_model_feature_matrix(
                augmented_df, feature_names
            )
            if missing:
                messages.append(
                    "å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ä½¿ç”¨ã—ãŸåˆ—ãŒä¸€éƒ¨è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: "
                    + ", ".join(missing[:5])
                    + ("..." if len(missing) > 5 else "")
                )
            try:
                resid_pred = pd.Series(
                    model.predict(feature_matrix),
                    index=feature_matrix.index,
                    dtype=float,
                )
            except Exception as exc:
                messages.append(f"CatBoostäºˆæ¸¬ã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}")
                resid_pred = pd.Series(0.0, index=df.index, dtype=float)
                feature_matrix = None
        else:
            messages.append("CatBoostãƒ¢ãƒ‡ãƒ«ã«ç‰¹å¾´é‡åãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    else:
        messages.append(
            "CatBoostãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã‚ãªã‹ã£ãŸãŸã‚ã€æ®‹å·®è£œæ­£ãªã—ã§è¡¨ç¤ºã—ã¾ã™ã€‚"
        )

    pred_delta = (
        base_pred + resid_pred
    )  # å¤‰åŒ–é‡ã®äºˆæ¸¬å€¤ = å¤‰åŒ–é‡äºˆæ¸¬ + æ®‹å·®ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹è£œæ­£å€¤
    pred_vacancy_2023 = vac18 + pred_delta  # 2023å¹´ã®ç©ºãå®¶ç‡ã®æœ€çµ‚äºˆæ¸¬å€¤
    residual_gap = delta_observed - pred_delta  # æœ€çµ‚çš„ãªå·®

    enriched = df.copy()
    enriched["Î”(23-18)"] = delta_observed
    enriched["baseline_pred_delta"] = base_pred
    enriched["residual_model_pred"] = resid_pred
    enriched["pred_delta"] = pred_delta
    enriched["pred_ç©ºãå®¶ç‡_2023"] = pred_vacancy_2023
    enriched["æ®‹å·®(å®Ÿæ¸¬-äºˆæ¸¬)"] = residual_gap

    return enriched, feature_matrix, messages


def compute_shap_topk(
    model: Optional[CatBoostRegressor],
    features: Optional[pd.DataFrame],
    k: int = 3,
    codes: Optional[pd.Series] = None,
) -> Tuple[dict[str, list[str]], Optional[str]]:
    if model is None or features is None or features.empty or Pool is None:
        return {}, None

    try:
        pool = Pool(features)
        shap_values = model.get_feature_importance(pool, type="ShapValues")
    except Exception as exc:  # pragma: no cover - defensive
        return {}, f"SHAPå€¤ã®è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}"

    shap_matrix = pd.DataFrame(
        shap_values[:, :-1], index=features.index, columns=features.columns
    )

    top_lookup: dict[str, list[str]] = {}
    codes_prepared: Optional[pd.Series] = None
    if codes is not None:
        codes_prepared = codes.astype(str).str.zfill(5)

    centroid_cols = {"centroid_lat_std", "centroid_lon_std"}

    for idx, row in shap_matrix.iterrows():
        if codes_prepared is not None and idx in codes_prepared.index:
            key = codes_prepared.loc[idx]
        else:
            key = str(idx)
        sorted_feats = row.abs().sort_values(ascending=False)
        top_candidates = sorted_feats.head(max(k, 5))
        filtered = [
            feat for feat in top_candidates.index if feat not in centroid_cols
        ]
        if len(filtered) >= k:
            selected = filtered[:k]
        else:
            # è¶³ã‚Šãªã„åˆ†ã¯å€™è£œãƒªã‚¹ãƒˆã‹ã‚‰è£œå……ï¼ˆcentroidåˆ—ã‚’å«ã‚€å ´åˆã‚ã‚Šï¼‰
            selected = filtered + [
                feat
                for feat in top_candidates.index
                if feat not in filtered
            ][: k - len(filtered)]

        formatted = [f"{feat}: {row[feat]:+.3f}" for feat in selected]
        top_lookup[key] = formatted
    return top_lookup, None


# ---------------------------------------------------------------------------
# Risk classification & geo utilities
# ---------------------------------------------------------------------------


# 4ãƒ¬ãƒ™ãƒ«ç©ºãå®¶ãƒªã‚¹ã‚¯ã®åˆ¤å®š
def classify_risk(df: pd.DataFrame, tolerance: float) -> pd.DataFrame:
    df = df.copy()
    vac18 = _get_numeric_series(df, "ç©ºãå®¶ç‡_2018")
    vac23 = _get_numeric_series(df, "ç©ºãå®¶ç‡_2023")
    delta = vac23 - vac18

    if vac23.dropna().empty:
        threshold = np.nan
    else:
        threshold = float(np.nanpercentile(vac23.dropna(), 75))

    trend = np.where(
        delta > tolerance, "å¢—", np.where(delta < -tolerance, "æ¸›", "æ¨ªã°ã„")
    )
    high_now = (
        vac23 >= threshold if not np.isnan(threshold) else np.full(len(vac23), False)
    )

    labels = []
    for is_high, trend_value in zip(high_now, trend):
        label = RISK_RULES.get((bool(is_high), trend_value), DEFAULT_RISK_LABEL)
        labels.append(label)

    df["P75_threshold"] = threshold
    df["Î”(23-18)"] = delta
    df["ãƒªã‚¹ã‚¯åŒºåˆ†"] = labels
    df["ãƒˆãƒ¬ãƒ³ãƒ‰"] = trend

    return df


def _format_shap_text(items: Iterable[str]) -> str:
    values = [item for item in items if item]
    if not values:
        return "SHAPæƒ…å ±ãªã—"
    return "<br/>".join(values)


def build_static_geojson(
    geojson: Optional[dict[str, Any]],
    df: pd.DataFrame,
    shap_lookup: dict[str, list[str]],
) -> Optional[dict[str, Any]]:
    if geojson is None:
        return None

    static_geojson = json.loads(json.dumps(geojson))
    dedup_df = (
        df.dropna(subset=["å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰"])
        .drop_duplicates(subset=["å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰"], keep="last")
        .set_index("å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰")
    )
    lookup = dedup_df.to_dict("index")

    for feature in static_geojson.get("features", []):
        props = feature.setdefault("properties", {})
        code = str(props.get("å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰", "")).zfill(5)
        record = lookup.get(code)
        props["akiya_code"] = code
        if record is None:
            props["akiya_has_data"] = False
            # å‹•çš„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚‚åˆæœŸåŒ–
            props.setdefault("akiya_name", "ä¸æ˜")
            props.setdefault("akiya_vac18", None)
            props.setdefault("akiya_vac23", None)
            props.setdefault("akiya_pred", None)
            props.setdefault("akiya_residual", None)
            props.setdefault("akiya_delta", None)
            props.setdefault("akiya_shap", "SHAPæƒ…å ±ãªã—")
            props.setdefault("akiya_risk_dynamic", DEFAULT_RISK_LABEL)
            props.setdefault("akiya_trend_dynamic", "")
            props.setdefault("akiya_map_dynamic", None)
            continue

        props["akiya_has_data"] = True
        props["akiya_name"] = record.get("å¸‚åŒºç”ºæ‘å", "ä¸æ˜")
        props["akiya_vac18"] = _safe_value(record.get("ç©ºãå®¶ç‡_2018"))
        props["akiya_vac23"] = _safe_value(record.get("ç©ºãå®¶ç‡_2023"))
        props["akiya_pred"] = _safe_value(record.get("pred_ç©ºãå®¶ç‡_2023"))
        props["akiya_residual"] = _safe_value(record.get("æ®‹å·®(å®Ÿæ¸¬-äºˆæ¸¬)"))
        props["akiya_delta"] = _safe_value(record.get("Î”(23-18)"))
        internal_key = str(record.get("_akiya_internal_index", code))
        shap_items = shap_lookup.get(internal_key, [])
        props["akiya_shap"] = _format_shap_text(shap_items)

        # Dynamic placeholders
        props["akiya_risk_dynamic"] = DEFAULT_RISK_LABEL
        props["akiya_trend_dynamic"] = ""
        props["akiya_map_dynamic"] = None

    return static_geojson


def update_geojson_dynamic_properties(
    geojson: Optional[dict[str, Any]],
    risk_lookup: dict[str, str],
    trend_lookup: dict[str, str],
    map_lookup: dict[str, Optional[float]],
) -> None:
    if geojson is None:
        return

    for feature in geojson.get("features", []):
        props = feature.setdefault("properties", {})
        code = props.get("akiya_code") or str(props.get("å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰", "")).zfill(5)
        risk = risk_lookup.get(code, DEFAULT_RISK_LABEL)
        trend = trend_lookup.get(code, "")
        value = None
        if map_lookup:
            value = map_lookup.get(code)
            if isinstance(value, float) and np.isnan(value):
                value = None
        props["akiya_risk_dynamic"] = risk
        props["akiya_trend_dynamic"] = trend
        props["akiya_map_dynamic"] = value


def _safe_value(value: Any) -> Optional[float]:
    try:
        if value is None or (isinstance(value, float) and np.isnan(value)):
            return None
        return float(value)
    except (TypeError, ValueError):
        return None


def build_map(
    geojson: Optional[dict[str, Any]],
    map_option: str,
    legend_name: str,
    risk_lookup: dict[str, str],
    trend_lookup: dict[str, str],
    map_lookup: dict[str, Optional[float]],
) -> Optional[folium.Map]:
    if geojson is None or folium is None or st_folium is None:
        return None
    # foliumãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ãƒãƒƒãƒ—ã‚’æº–å‚™ (åˆæœŸè¡¨ç¤ºã¯æ—¥æœ¬å›½åˆ—å³¶)
    m = folium.Map(location=[35.6, 137.8], zoom_start=8, tiles="cartodbpositron")

    # ãƒ„ãƒ¼ãƒ«ãƒãƒƒãƒ—è¨­å®šï¼ˆé™çš„GeoJSONã«å«ã¾ã‚Œã‚‹ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ï¼‰
    def tooltip_fields() -> list[str]:
        return [
            "akiya_name",
            "akiya_vac18",
            "akiya_vac23",
            "akiya_delta",
            "akiya_risk_dynamic",
            "akiya_trend_dynamic",
        ]

    # ãƒ„ãƒ¼ãƒ«ãƒãƒƒãƒ—è¨­å®š (æ—¥æœ¬èªè¡¨ç¤ºå)
    tooltip = features.GeoJsonTooltip(
        fields=tooltip_fields(),
        aliases=[
            "è‡ªæ²»ä½“",
            "ç©ºãå®¶ç‡(2018)%",
            "ç©ºãå®¶ç‡(2023)%",
            "Î”(23-18)pt",
            "ãƒªã‚¹ã‚¯åŒºåˆ†",
            "ãƒˆãƒ¬ãƒ³ãƒ‰",
        ],
        localize=True,
        sticky=False,
    )

    # åœ°å›³ã®è‰²åˆ†ã‘ã”ã¨ã«è¨­å®š
    if map_option == "4æ®µéšãƒªã‚¹ã‚¯":
        color_map = {label: info["color"] for label, info in RISK_LEVELS.items()}

        def style_function(feature: dict[str, Any]) -> dict[str, Any]:
            props = feature.get("properties", {})
            if not props.get("akiya_has_data"):
                return {
                    "fillColor": NAN_COLOR,
                    "color": "#666666",
                    "weight": 0.2,
                    "fillOpacity": 0.5,
                }
            code = props.get("akiya_code") or str(
                props.get("å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰", "")
            ).zfill(5)
            risk = risk_lookup.get(code, DEFAULT_RISK_LABEL)
            color = color_map.get(risk, NAN_COLOR)
            return {
                "fillColor": color,
                "color": "#4d4d4d",
                "weight": 0.4,
                "fillOpacity": 0.75,
            }

        # GeoJSONãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’åœ°å›³ã«è¿½åŠ ã€‚ä¸Šè¨˜ã®style_functionã‚’é©ç”¨
        folium.GeoJson(
            geojson,
            style_function=style_function,
            highlight_function=lambda _: {"weight": 1.5, "color": "#000000"},
            tooltip=tooltip,
        ).add_to(m)

        # HTMLã§è‡ªä½œã—ãŸå‡¡ä¾‹ã‚’åœ°å›³ã®å·¦ä¸‹ã«è¿½åŠ 
        if Template is not None and MacroElement is not None:
            legend_items_html = "".join(
                [
                    (
                        '<div style="display:flex; align-items:center; margin-bottom:4px;">'
                        f"<span style=\"display:inline-block;width:14px;height:14px;background:{RISK_LEVELS[label]['color']};border:1px solid #555;\"></span>"
                        f"<span style=\"margin-left:8px;\">{RISK_LEVELS[label]['legend']}</span>"
                        "</div>"
                    )
                    for label in RISK_LEGEND_ORDER
                ]
            )
            legend_template = """
            {% macro html(this, kwargs) %}
            <div style="position: fixed; bottom: 30px; left: 30px; width: 230px;
                        background-color: #ffffff; border: 1px solid #999999;
                        z-index: 9999; font-size: 13px; padding: 10px;
                        color: #333333; box-shadow: 2px 2px 4px rgba(0,0,0,0.25);">
              <b style="display:block; margin-bottom:6px;">4æ®µéšãƒªã‚¹ã‚¯</b>
              __ITEMS__
            </div>
            {% endmacro %}
            """
            legend_template = legend_template.replace("__ITEMS__", legend_items_html)
            legend = MacroElement()
            legend._template = Template(legend_template)
            m.get_root().add_child(legend)
    # --- ã‚±ãƒ¼ã‚¹2: ãã‚Œä»¥å¤–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆç©ºãå®¶ç‡ãªã©ï¼‰ãŒé¸æŠã•ã‚ŒãŸå ´åˆ ---
    else:
        values = [
            v
            for v in map_lookup.values()
            if v is not None and not (isinstance(v, float) and np.isnan(v))
        ]
        if not values or branca_linear is None:
            colormap = None
        else:
            vmin, vmax = float(np.min(values)), float(np.max(values))
            if np.isclose(vmin, vmax):
                vmax = vmin + 1e-6
            colormap = branca_linear.YlOrRd_09.scale(vmin, vmax)  # ã‚°ãƒ©ãƒ‡è¡¨ç¤º
            colormap.caption = legend_name
            colormap.add_to(m)  # åœ°å›³ã«å‡¡ä¾‹ï¼ˆã‚«ãƒ©ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰ã‚’è¿½åŠ 

        # å„å¸‚åŒºç”ºæ‘ã‚’ã©ã®ã‚ˆã†ã«è‰²ä»˜ã‘ã™ã‚‹ã‹ã®è¨­å®š
        def style_function(feature: dict[str, Any]) -> dict[str, Any]:
            props = feature.get("properties", {})
            if not props.get("akiya_has_data"):
                return {
                    "fillColor": NAN_COLOR,
                    "color": "#666666",
                    "weight": 0.2,
                    "fillOpacity": 0.5,
                }
            code = props.get("akiya_code") or str(
                props.get("å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰", "")
            ).zfill(5)
            value = map_lookup.get(code)
            if (
                value is None
                or (isinstance(value, float) and np.isnan(value))
                or colormap is None
            ):
                color = NAN_COLOR
            else:
                color = colormap(value)
            return {
                "fillColor": color,
                "color": "#4d4d4d",
                "weight": 0.4,
                "fillOpacity": 0.75,
            }

        # GeoJSONãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’åœ°å›³ã«è¿½åŠ 
        folium.GeoJson(
            geojson,
            style_function=style_function,
            highlight_function=lambda _: {"weight": 1.5, "color": "#000000"},
            tooltip=tooltip,
        ).add_to(m)

    return m


# ---------------------------------------------------------------------------
# Streamlit layout
# ---------------------------------------------------------------------------


def main() -> None:
    st.set_page_config(page_title="Akiya-Lens", layout="wide")
    st.title("ğŸ  Akiya-Lens æ®‹å·®ãƒ¢ãƒ‡ãƒ«ãƒ“ãƒ¥ãƒ¼ã‚¢")
    st.caption("2018â†’2023 ç©ºãå®¶ç‡ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ + æ®‹å·®ãƒ¢ãƒ‡ãƒ«è§£æ")

    try:
        features_df = load_features(DATA_PATH)
    except FileNotFoundError as exc:
        st.error(str(exc))
        return

    geojson = load_geojson(GEOJSON_PATH)
    metrics = load_model_metrics(METRICS_PATH)
    model = load_catboost_model(MODEL_PATH)

    with st.sidebar:
        st.header("è¡¨ç¤ºè¨­å®š")
        tol = st.slider("æ¨ªã°ã„è¨±å®¹å¹… (ï¼…ãƒã‚¤ãƒ³ãƒˆ)", 0.0, 1.0, DEFAULT_TOLERANCE, 0.05)
        map_option = st.selectbox("åœ°å›³ã®è‰²åˆ†ã‘", MAP_OPTIONS)
        if model is None:
            st.warning(
                "CatBoostãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ã¿ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚"
            )
        if metrics:
            st.subheader("ãƒ¢ãƒ‡ãƒ«æŒ‡æ¨™")
            cat_metrics = metrics.get("catboost")
            if cat_metrics:
                st.metric("CatBoost RÂ²", f"{cat_metrics.get('r2', np.nan):.3f}")
                st.metric("CatBoost MSE", f"{cat_metrics.get('mse', np.nan):.3f}")

    if "model_cache" not in st.session_state:
        enriched_once, feature_matrix, base_messages = compute_predictions(
            features_df, model
        )
        if feature_matrix is not None and "å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰" in features_df.columns:
            code_series = features_df.loc[feature_matrix.index, "å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰"]
        else:
            code_series = None
        shap_lookup_once, shap_message_once = compute_shap_topk(
            model, feature_matrix, codes=code_series
        )
        cache_messages = base_messages.copy()
        if shap_message_once:
            cache_messages.append(shap_message_once)
        static_geojson = build_static_geojson(geojson, enriched_once, shap_lookup_once)
        st.session_state["model_cache"] = {
            "enriched_df": enriched_once,
            "shap_lookup": shap_lookup_once,
            "geojson_static": static_geojson,
            "messages": cache_messages,
        }

    cache = st.session_state["model_cache"]
    enriched_df = cache["enriched_df"].copy()
    shap_lookup = cache["shap_lookup"]
    static_geojson = cache.get("geojson_static")
    if static_geojson is None and geojson is not None:
        static_geojson = build_static_geojson(geojson, enriched_df, shap_lookup)
        cache["geojson_static"] = static_geojson
    info_messages = list(cache.get("messages", []))

    classified_df = classify_risk(enriched_df, tol)

    risk_lookup = (
        classified_df.set_index("å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰")["ãƒªã‚¹ã‚¯åŒºåˆ†"].to_dict()
        if "å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰" in classified_df.columns
        else {}
    )
    trend_lookup = (
        classified_df.set_index("å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰")["ãƒˆãƒ¬ãƒ³ãƒ‰"].to_dict()
        if "å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰" in classified_df.columns
        else {}
    )

    if map_option == "2023å¹´ç©ºãå®¶ç‡ï¼ˆå®Ÿæ¸¬ï¼‰":
        map_lookup = (
            classified_df.set_index("å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰")["ç©ºãå®¶ç‡_2023"].to_dict()
            if "å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰" in classified_df.columns
            else {}
        )
        legend = "2023å¹´ç©ºãå®¶ç‡ (%)"
    elif map_option == "2023å¹´ç©ºãå®¶ç‡ï¼ˆäºˆæ¸¬ï¼‰":
        map_lookup = (
            classified_df.set_index("å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰")["pred_ç©ºãå®¶ç‡_2023"].to_dict()
            if "å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰" in classified_df.columns
            else {}
        )
        legend = "äºˆæ¸¬ç©ºãå®¶ç‡ (%)"
    elif map_option == "æ®‹å·®ï¼ˆå®Ÿæ¸¬âˆ’äºˆæ¸¬ï¼‰":
        map_lookup = (
            classified_df.set_index("å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰")["æ®‹å·®(å®Ÿæ¸¬-äºˆæ¸¬)"].to_dict()
            if "å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰" in classified_df.columns
            else {}
        )
        legend = "æ®‹å·® (pt)"
    else:
        map_lookup = {}
        legend = ""

    update_geojson_dynamic_properties(
        static_geojson, risk_lookup, trend_lookup, map_lookup
    )
    folium_map = build_map(
        static_geojson, map_option, legend, risk_lookup, trend_lookup, map_lookup
    )

    map_col, inspector_col = st.columns((2.2, 1.0))

    with map_col:
        if folium_map is None:
            st.warning(
                "Folium ã¾ãŸã¯ GeoJSON ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€åœ°å›³ã¯è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚"
            )
            map_state = {}
        else:
            map_state = st_folium(
                folium_map,
                height=650,
                use_container_width=True,
                key="akiya_map",
            )

    selected_code = st.session_state.get("selected_code")
    props = None
    if map_state:
        drawing = map_state.get("last_active_drawing") or map_state.get(
            "last_object_clicked"
        )
        if drawing and isinstance(drawing, dict):
            props = drawing.get("properties", {})
    if props:
        code = props.get("akiya_code") or props.get("å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰")
        if code:
            selected_code = str(code).zfill(5)
            st.session_state["selected_code"] = selected_code

    with inspector_col:
        st.subheader("è‡ªæ²»ä½“ã‚¤ãƒ³ã‚¹ãƒšã‚¯ã‚¿")
        if not selected_code:
            st.info("åœ°å›³ä¸Šã®è‡ªæ²»ä½“ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨è©³ç´°ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
        else:
            row = classified_df[
                classified_df["å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰"].astype(str).str.zfill(5)
                == selected_code
            ]
            if row.empty:
                st.warning("é¸æŠã—ãŸè‡ªæ²»ä½“ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            else:
                record = row.iloc[0]
                st.markdown(f"### {record.get('å¸‚åŒºç”ºæ‘å', 'ä¸æ˜')} ({selected_code})")
                st.markdown(
                    f"- **ç©ºãå®¶ç‡** 2018: {record.get('ç©ºãå®¶ç‡_2018', np.nan):.2f}% / 2023: {record.get('ç©ºãå®¶ç‡_2023', np.nan):.2f}%"
                )
                st.markdown(
                    f"- **Î”(23-18):** {record.get('Î”(23-18)', np.nan):.2f} pt | **ãƒªã‚¹ã‚¯:** {risk_lookup.get(selected_code, DEFAULT_RISK_LABEL)} | **ãƒˆãƒ¬ãƒ³ãƒ‰:** {trend_lookup.get(selected_code, '')}"
                )
                st.markdown(
                    f"- **äºˆæ¸¬ç©ºãå®¶ç‡(2023):** {record.get('pred_ç©ºãå®¶ç‡_2023', np.nan):.2f}% | **æ®‹å·®:** {record.get('æ®‹å·®(å®Ÿæ¸¬-äºˆæ¸¬)', np.nan):.2f} pt"
                )
                top_factors = shap_lookup.get(selected_code)
                st.markdown("- **è¦å› Top3**")
                if top_factors:
                    for item in top_factors:
                        st.markdown(f"- {item}")
                else:
                    st.markdown("  - SHAPæƒ…å ±ãªã—")

    st.subheader("è‡ªæ²»ä½“åˆ¥ãƒ†ãƒ¼ãƒ–ãƒ«")
    table_cols = [
        "å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰",
        "å¸‚åŒºç”ºæ‘å",
        "éƒ½é“åºœçœŒå",
        "ç©ºãå®¶ç‡_2018",
        "ç©ºãå®¶ç‡_2023",
        "Î”(23-18)",
        "ãƒªã‚¹ã‚¯åŒºåˆ†",
        "ãƒˆãƒ¬ãƒ³ãƒ‰",
        "pred_ç©ºãå®¶ç‡_2023",
        "æ®‹å·®(å®Ÿæ¸¬-äºˆæ¸¬)",
    ]
    existing_cols = [col for col in table_cols if col in classified_df.columns]
    table_data = classified_df[existing_cols].copy()
    if "ãƒªã‚¹ã‚¯åŒºåˆ†" in table_data.columns:
        table_data = table_data.sort_values("ãƒªã‚¹ã‚¯åŒºåˆ†")
    st.dataframe(table_data, use_container_width=True)

    if info_messages:
        st.subheader("ãƒ­ã‚° / æ³¨æ„äº‹é …")
        for msg in info_messages:
            if not msg:
                continue
            st.markdown(f"- {msg}")

    st.caption("ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: data/processed/features_master__wide__v1.csv ä»–")


if __name__ == "__main__":
    main()
