"""Streamlit interface for the Akiya-Lens residual model.

ã“ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯ ``docs/ãƒ¢ãƒ‡ãƒ«ã¨ã‚¢ãƒ—ãƒªè¨­è¨ˆä»•æ§˜.md`` ã®ä»•æ§˜ã«å¾“ã„ã¾ã™ã€‚
ãƒã‚¹ã‚¿ãƒ¼ç‰¹å¾´é‡ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€
ãƒªã‚¹ã‚¯ã‚«ãƒ†ã‚´ãƒªã€äºˆæ¸¬ã€SHAPç”±æ¥ã®èª¬æ˜ã‚’å«ã‚€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªå¸‚ç”ºæ‘ãƒãƒƒãƒ—ã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚

"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

import numpy as np
import pandas as pd
import streamlit as st

from dotenv import load_dotenv

load_dotenv(override=False)  # .env â†’ os.environ ã¸èª­ã¿è¾¼ã¿

try:
    import folium
    from folium import features
    from branca.colormap import linear as branca_linear
    from branca.element import MacroElement, Template
    from streamlit_folium import st_folium
except ImportError:  # pragma: no cover - handled gracefully in UI
    folium = None
    features = None
    branca_linear = None
    MacroElement = None
    Template = None
    st_folium = None

# ==== add: Groq client ====
try:
    from groq import Groq
except ImportError:  # pragma: no cover
    Groq = None

import os
import hashlib
import textwrap


try:
    from catboost import CatBoostRegressor, Pool
except ImportError:  # pragma: no cover - handled gracefully in UI
    CatBoostRegressor = None  # type: ignore
    Pool = None  # type: ignore

from scripts.models.diff_model_utils import (
    compute_predictions,
    compute_shap_topk,
    get_numeric_series,
)


# ======= UI: CSS / helpers =======
def inject_css() -> None:
    import streamlit as st

    st.markdown(
        """
    <style>
    :root {
       --card-bg: #ffffff;
       --muted: #6b7280;
       --border: #e5e7eb;
    }
    .akiya-card {
       border: 1px solid var(--border);
       border-radius: 14px;
       padding: 14px 16px;
       background: var(--card-bg);
       box-shadow: 0 1px 2px rgba(0,0,0,.06);
       margin-bottom: 10px;
    }
    .akiya-title { font-weight:700; font-size:1.1rem; margin-bottom:6px;}
    .akiya-subtle { color: var(--muted); font-size: .9rem; }
    .akiya-badges { display:flex; flex-wrap: wrap; gap:6px; margin:.25rem 0 .5rem; }
    .akiya-pill { padding: 2px 10px; border-radius: 999px; font-size:.78rem; border: 1px solid var(--border); background:#f3f4f6;}
    .small-metric .stMetric { font-size: 12px !important; }
    .small-metric [data-testid="stMetricValue"] { font-size: 14px !important; color: var(--muted);}
    .akiya-muted { color: var(--muted); }
    .akiya-log li{ color:#6b7280;}
    </style>
    """,
        unsafe_allow_html=True,
    )


def risk_badge(label: str) -> str:
    color = RISK_LEVELS.get(label, {}).get("color", "#999999")
    text = label.replace("(", "").replace(")", "")
    return (
        f'<span class="akiya-pill" '
        f'style="background:{color}22;border-color:{color}55;color:{color}">{text}</span>'
    )


# ---------------------------------------------------------------------------
# Paths & constants
# ---------------------------------------------------------------------------

REPO_ROOT = Path(__file__).resolve().parent
DATA_PATH = (
    REPO_ROOT / "data/processed/features_master__wide__v1.csv"
)  # å¸‚åŒºç”ºæ‘ãƒ‡ãƒ¼ã‚¿(ç‰¹å¾´é‡)
GEOJSON_PATH = (
    REPO_ROOT / "data/geojson/municipalities_simplified.geojson"
)  # åœ°å›³ãƒ‡ãƒ¼ã‚¿(geojson)
MODEL_PATH = REPO_ROOT / "models/final_diff_model.cbm"  # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«(CatBoost)
METRICS_PATH = REPO_ROOT / "data/processed/model_metrics.json"  # è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
INSPECTOR_PATH = REPO_ROOT / "data/processed/diff_model_inspector.json"

AI_CACHE_DIR = (
    REPO_ROOT / "data/processed" / "ai_cache"
)  # å†å­¦ç¿’ã§ç½²åãŒå¤‰ã‚ã‚‹ã¾ã§ç”ŸæˆAIçµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥

DEFAULT_TOLERANCE = 0.1  # "æ¨ªã°ã„"ã¨ã™ã‚‹é–¾å€¤ (ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼UIã§èª¿æ•´å¯)
# å¸‚åŒºç”ºæ‘å¡—ã‚Šã¤ã¶ã—ã‚»ãƒ¬ã‚¯ãƒˆ
MAP_OPTIONS = (
    "4æ®µéšãƒªã‚¹ã‚¯",
    "2023å¹´ç©ºãå®¶ç‡ï¼ˆå®Ÿæ¸¬ï¼‰",
    "2023å¹´ç©ºãå®¶ç‡ï¼ˆäºˆæ¸¬ï¼‰",
)
# ãƒªã‚¹ã‚¯ãƒ©ãƒ™ãƒ«å®šç¾©ï¼ˆè‰²ãƒ»å‡¡ä¾‹ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€å…ƒç®¡ç†ï¼‰
RISK_LEVELS = {
    "(æœ€å„ªå…ˆ)": {"color": "#d73027", "legend": "èµ¤(æœ€å„ªå…ˆ) é«˜ãƒ»âš"},
    "(æ³¨æ„)": {"color": "#fc8d59", "legend": "ã‚ªãƒ¬ãƒ³ã‚¸(æ³¨æ„) é«˜ãƒ»â™"},
    "(è­¦æˆ’)": {"color": "#fee08b", "legend": "é»„(è­¦æˆ’) ä½ãƒ»âš"},
    "(ä½)": {"color": "#1a9850", "legend": "ç·‘(ä½) ä½ãƒ»â™"},
}

# é«˜ä½Ã—ãƒˆãƒ¬ãƒ³ãƒ‰ã®çµ„åˆã›ã”ã¨ã®ãƒ©ãƒ™ãƒ«å‰²ã‚Šå½“ã¦
RISK_RULES = {
    (True, "å¢—"): "(æœ€å„ªå…ˆ)",
    (True, "æ¨ªã°ã„"): "(æ³¨æ„)",
    (True, "æ¸›"): "(æ³¨æ„)",
    (False, "å¢—"): "(è­¦æˆ’)",
    (False, "æ¨ªã°ã„"): "(ä½)",
    (False, "æ¸›"): "(ä½)",
}

RISK_LEGEND_ORDER = ["(æœ€å„ªå…ˆ)", "(æ³¨æ„)", "(è­¦æˆ’)", "(ä½)"]
DEFAULT_RISK_LABEL = "(ä½)"

NAN_COLOR = "#d9d9d9"

# --- ç”ŸæˆAIæ–‡è¨€ Groq å…±é€šæ³¨æ„æ–‡ã‚’å®šç¾© ---
COMMON_FIXED_NOTES = [
    "ç©ºãå®¶ç‡ã®å¢—åŠ ã¯ã€è¤‡æ•°ã®è¦å› ã«ã‚ˆã‚‹çµæœã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€å˜ä¸€ã®å¯¾ç­–ã§ã¯è§£æ±ºã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚‹",
    "SHAPåˆ†æã¯ã€ç‰¹å®šã®è¦å› ã®å¯„ä¸åº¦ã‚’ç®—å‡ºã™ã‚‹ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã®åã‚Šã‚„æ¬ é™¥ãŒå½±éŸ¿ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹",
]


# ---------------------------------------------------------------------------
# Data loading utilities
# ---------------------------------------------------------------------------


def _normalise_columns(df: pd.DataFrame) -> pd.DataFrame:
    df.columns = [c.lstrip("\ufeff") for c in df.columns]  # BOMé™¤å»
    return df


@st.cache_data(show_spinner=False)  # "å®Ÿè¡Œä¸­â€¦ã ã•ãªã„"
def load_features(path: Path) -> pd.DataFrame:
    if not path.exists():
        raise FileNotFoundError(f"ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
    df = pd.read_csv(path)
    df = _normalise_columns(df)
    df["å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰"] = (
        df["å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰"]
        .astype(str)
        .str.replace(r"\.0$", "", regex=True)
        .str.zfill(5)  # éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰1ã€œ9ã‚’0åŸ‹ã‚
    )
    return df


@st.cache_data(show_spinner=False)
def load_geojson(path: Path) -> Optional[dict[str, Any]]:
    if not path.exists():
        return None
    with path.open("r", encoding="utf-8") as handle:
        return json.load(handle)


@st.cache_data(show_spinner=False)
def load_model_metrics(path: Path) -> Dict[str, Any]:
    if not path.exists():
        return {}
    with path.open("r", encoding="utf-8") as handle:
        return json.load(handle)


@st.cache_data(show_spinner=False)
def load_inspector_payload(path: Path, _signature: float) -> Dict[str, Any]:
    if not path.exists():
        return {}
    try:
        with path.open("r", encoding="utf-8") as handle:
            data = json.load(handle)
            if isinstance(data, dict):
                return data
    except json.JSONDecodeError:
        pass
    return {}


@st.cache_resource(show_spinner=False)
def load_catboost_model(path: Path) -> Optional[CatBoostRegressor]:
    if CatBoostRegressor is None:
        return None
    if not path.exists():
        return None
    model = CatBoostRegressor()
    model.load_model(str(path))
    return model


# ---------------------------------------------------------------------------
# Risk classification & geo utilities
# ---------------------------------------------------------------------------


# 4ãƒ¬ãƒ™ãƒ«ç©ºãå®¶ãƒªã‚¹ã‚¯ã®åˆ¤å®š
def classify_risk(df: pd.DataFrame, tolerance: float) -> pd.DataFrame:
    df = df.copy()
    vac18 = get_numeric_series(df, "ç©ºãå®¶ç‡_2018")
    vac23 = get_numeric_series(df, "ç©ºãå®¶ç‡_2023")
    delta = vac23 - vac18

    if vac23.dropna().empty:
        threshold = np.nan
    else:
        threshold = float(np.nanpercentile(vac23.dropna(), 75))

    trend = np.where(
        delta > tolerance, "å¢—", np.where(delta < -tolerance, "æ¸›", "æ¨ªã°ã„")
    )
    high_now = (
        vac23 >= threshold if not np.isnan(threshold) else np.full(len(vac23), False)
    )

    labels = []
    for is_high, trend_value in zip(high_now, trend):
        label = RISK_RULES.get((bool(is_high), trend_value), DEFAULT_RISK_LABEL)
        labels.append(label)

    df["P75_threshold"] = threshold
    df["Î”(23-18)"] = delta
    df["ãƒªã‚¹ã‚¯åŒºåˆ†"] = labels
    df["ãƒˆãƒ¬ãƒ³ãƒ‰"] = trend

    return df


def _format_shap_text(items: Iterable[str]) -> str:
    values = [item for item in items if item]
    if not values:
        return "SHAPæƒ…å ±ãªã—"
    return "<br/>".join(values)


def build_static_geojson(
    geojson: Optional[dict[str, Any]],
    df: pd.DataFrame,
    shap_lookup: dict[str, list[str]],
    tooltip_lookup: Optional[dict[str, Dict[str, Any]]] = None,
) -> Optional[dict[str, Any]]:
    if geojson is None:
        return None

    static_geojson = json.loads(json.dumps(geojson))
    dedup_df = (
        df.dropna(subset=["å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰"])
        .drop_duplicates(subset=["å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰"], keep="last")
        .set_index("å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰")
    )
    lookup = dedup_df.to_dict("index")

    for feature in static_geojson.get("features", []):
        props = feature.setdefault("properties", {})
        code = str(props.get("å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰", "")).zfill(5)
        record = lookup.get(code)
        props["akiya_code"] = code
        if record is None:
            props["akiya_has_data"] = False
            # å‹•çš„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚‚åˆæœŸåŒ–
            props.setdefault("akiya_name", "ä¸æ˜")
            props.setdefault("akiya_vac18", None)
            props.setdefault("akiya_vac23", None)
            props.setdefault("akiya_pred", None)
            props.setdefault("akiya_residual", None)
            props.setdefault("akiya_delta", None)
            props.setdefault("akiya_shap", "SHAPæƒ…å ±ãªã—")
            props.setdefault("akiya_risk_dynamic", DEFAULT_RISK_LABEL)
            props.setdefault("akiya_trend_dynamic", "")
            props.setdefault("akiya_map_dynamic", None)
            continue

        props["akiya_has_data"] = True
        tooltip_info = tooltip_lookup.get(code) if tooltip_lookup else None
        props["akiya_name"] = (
            tooltip_info.get("akiya_name")
            if isinstance(tooltip_info, dict) and tooltip_info.get("akiya_name")
            else record.get("å¸‚åŒºç”ºæ‘å", "ä¸æ˜")
        )
        props["akiya_vac18"] = _safe_value(
            tooltip_info.get("akiya_vac18")
            if isinstance(tooltip_info, dict) and "akiya_vac18" in tooltip_info
            else record.get("ç©ºãå®¶ç‡_2018")
        )
        props["akiya_vac23"] = _safe_value(
            tooltip_info.get("akiya_vac23")
            if isinstance(tooltip_info, dict) and "akiya_vac23" in tooltip_info
            else record.get("ç©ºãå®¶ç‡_2023")
        )
        props["akiya_pred"] = _safe_value(record.get("pred_ç©ºãå®¶ç‡_2023"))
        props["akiya_residual"] = _safe_value(record.get("â–³(å®Ÿæ¸¬-äºˆæ¸¬)"))
        props["akiya_delta"] = _safe_value(
            tooltip_info.get("akiya_delta")
            if isinstance(tooltip_info, dict) and "akiya_delta" in tooltip_info
            else record.get("Î”(23-18)")
        )
        internal_key = str(record.get("_akiya_internal_index", code))
        shap_items = shap_lookup.get(internal_key, [])
        props["akiya_shap"] = _format_shap_text(shap_items)

        # Dynamic placeholders
        props["akiya_risk_dynamic"] = DEFAULT_RISK_LABEL
        props["akiya_trend_dynamic"] = ""
        props["akiya_map_dynamic"] = None

    return static_geojson


def update_geojson_dynamic_properties(
    geojson: Optional[dict[str, Any]],
    risk_lookup: dict[str, str],
    trend_lookup: dict[str, str],
    map_lookup: dict[str, Optional[float]],
) -> None:
    if geojson is None:
        return

    for feature in geojson.get("features", []):
        props = feature.setdefault("properties", {})
        code = props.get("akiya_code") or str(props.get("å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰", "")).zfill(5)
        risk = risk_lookup.get(code, DEFAULT_RISK_LABEL)
        trend = trend_lookup.get(code, "")
        value = None
        if map_lookup:
            value = map_lookup.get(code)
            if isinstance(value, float) and np.isnan(value):
                value = None
        props["akiya_risk_dynamic"] = risk
        props["akiya_trend_dynamic"] = trend
        props["akiya_map_dynamic"] = value


def _safe_value(value: Any) -> Optional[float]:
    try:
        if value is None or (isinstance(value, float) and np.isnan(value)):
            return None
        return float(value)
    except (TypeError, ValueError):
        return None


def build_map(
    geojson: Optional[dict[str, Any]],
    map_option: str,
    legend_name: str,
    risk_lookup: dict[str, str],
    trend_lookup: dict[str, str],
    map_lookup: dict[str, Optional[float]],
) -> Optional[folium.Map]:
    if geojson is None or folium is None or st_folium is None:
        return None
    # foliumãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ãƒãƒƒãƒ—ã‚’æº–å‚™ (åˆæœŸè¡¨ç¤ºã¯æ—¥æœ¬å›½åˆ—å³¶)
    m = folium.Map(location=[35.6, 137.8], zoom_start=8, tiles="cartodbpositron")

    # ãƒ„ãƒ¼ãƒ«ãƒãƒƒãƒ—è¨­å®šï¼ˆé™çš„GeoJSONã«å«ã¾ã‚Œã‚‹ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ï¼‰
    def tooltip_fields() -> list[str]:
        return [
            "akiya_name",
            "akiya_vac18",
            "akiya_vac23",
            "akiya_delta",
            "akiya_risk_dynamic",
            "akiya_trend_dynamic",
        ]

    # ãƒ„ãƒ¼ãƒ«ãƒãƒƒãƒ—è¨­å®š (æ—¥æœ¬èªè¡¨ç¤ºå)
    tooltip = features.GeoJsonTooltip(
        fields=tooltip_fields(),
        aliases=[
            "è‡ªæ²»ä½“",
            "ç©ºãå®¶ç‡(2018)%",
            "ç©ºãå®¶ç‡(2023)%",
            "Î”(23-18)pt",
            "ãƒªã‚¹ã‚¯åŒºåˆ†",
            "ãƒˆãƒ¬ãƒ³ãƒ‰",
        ],
        localize=True,
        sticky=False,
    )

    # åœ°å›³ã®è‰²åˆ†ã‘ã”ã¨ã«è¨­å®š
    if map_option == "4æ®µéšãƒªã‚¹ã‚¯":
        color_map = {label: info["color"] for label, info in RISK_LEVELS.items()}

        def style_function(feature: dict[str, Any]) -> dict[str, Any]:
            props = feature.get("properties", {})
            if not props.get("akiya_has_data"):
                return {
                    "fillColor": NAN_COLOR,
                    "color": "#666666",
                    "weight": 0.2,
                    "fillOpacity": 0.5,
                }
            code = props.get("akiya_code") or str(
                props.get("å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰", "")
            ).zfill(5)
            risk = risk_lookup.get(code, DEFAULT_RISK_LABEL)
            color = color_map.get(risk, NAN_COLOR)
            return {
                "fillColor": color,
                "color": "#4d4d4d",
                "weight": 0.4,
                "fillOpacity": 0.75,
            }

        # GeoJSONãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’åœ°å›³ã«è¿½åŠ ã€‚ä¸Šè¨˜ã®style_functionã‚’é©ç”¨
        folium.GeoJson(
            geojson,
            style_function=style_function,
            highlight_function=lambda _: {"weight": 1.5, "color": "#000000"},
            tooltip=tooltip,
        ).add_to(m)

        # HTMLã§è‡ªä½œã—ãŸå‡¡ä¾‹ã‚’åœ°å›³ã®å·¦ä¸‹ã«è¿½åŠ 
        if Template is not None and MacroElement is not None:
            legend_items_html = "".join(
                [
                    (
                        '<div style="display:flex; align-items:center; margin-bottom:4px;">'
                        f"<span style=\"display:inline-block;width:14px;height:14px;background:{RISK_LEVELS[label]['color']};border:1px solid #555;\"></span>"
                        f"<span style=\"margin-left:8px;\">{RISK_LEVELS[label]['legend']}</span>"
                        "</div>"
                    )
                    for label in RISK_LEGEND_ORDER
                ]
            )
            legend_template = """
            {% macro html(this, kwargs) %}
            <div style="position: fixed; bottom: 30px; left: 30px; width: 230px;
                        background-color: #ffffff; border: 1px solid #999999;
                        z-index: 9999; font-size: 13px; padding: 10px;
                        color: #333333; box-shadow: 2px 2px 4px rgba(0,0,0,0.25);">
              <b style="display:block; margin-bottom:6px;">4æ®µéšãƒªã‚¹ã‚¯</b>
              __ITEMS__
            </div>
            {% endmacro %}
            """
            legend_template = legend_template.replace("__ITEMS__", legend_items_html)
            legend = MacroElement()
            legend._template = Template(legend_template)
            m.get_root().add_child(legend)
    # --- ã‚±ãƒ¼ã‚¹2: ãã‚Œä»¥å¤–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆç©ºãå®¶ç‡ãªã©ï¼‰ãŒé¸æŠã•ã‚ŒãŸå ´åˆ ---
    else:
        values = [
            v
            for v in map_lookup.values()
            if v is not None and not (isinstance(v, float) and np.isnan(v))
        ]
        if not values or branca_linear is None:
            colormap = None
        else:
            vmin, vmax = float(np.min(values)), float(np.max(values))
            if np.isclose(vmin, vmax):
                vmax = vmin + 1e-6
            colormap = branca_linear.YlOrRd_09.scale(vmin, vmax)  # ã‚°ãƒ©ãƒ‡è¡¨ç¤º
            colormap.caption = legend_name
            colormap.add_to(m)  # åœ°å›³ã«å‡¡ä¾‹ï¼ˆã‚«ãƒ©ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰ã‚’è¿½åŠ 

        # å„å¸‚åŒºç”ºæ‘ã‚’ã©ã®ã‚ˆã†ã«è‰²ä»˜ã‘ã™ã‚‹ã‹ã®è¨­å®š
        def style_function(feature: dict[str, Any]) -> dict[str, Any]:
            props = feature.get("properties", {})
            if not props.get("akiya_has_data"):
                return {
                    "fillColor": NAN_COLOR,
                    "color": "#666666",
                    "weight": 0.2,
                    "fillOpacity": 0.5,
                }
            code = props.get("akiya_code") or str(
                props.get("å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰", "")
            ).zfill(5)
            value = map_lookup.get(code)
            if (
                value is None
                or (isinstance(value, float) and np.isnan(value))
                or colormap is None
            ):
                color = NAN_COLOR
            else:
                color = colormap(value)
            return {
                "fillColor": color,
                "color": "#4d4d4d",
                "weight": 0.4,
                "fillOpacity": 0.75,
            }

        # GeoJSONãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’åœ°å›³ã«è¿½åŠ 
        folium.GeoJson(
            geojson,
            style_function=style_function,
            highlight_function=lambda _: {"weight": 1.5, "color": "#000000"},
            tooltip=tooltip,
        ).add_to(m)

    return m


# ---------------------------------------------------------------------------
# Streamlit layout
# ---------------------------------------------------------------------------


def main() -> None:
    st.set_page_config(page_title="Akiya-Lens", layout="wide")
    inject_css()
    st.title("ğŸ  Akiya-Lens ç©ºãå®¶ç‡ãƒ¢ãƒ‡ãƒ«ãƒ“ãƒ¥ãƒ¼ã‚¢")
    st.caption("2018â†’2023 ç©ºãå®¶ã®è¦å› åˆ†æMAP")

    try:
        features_df = load_features(DATA_PATH)
    except FileNotFoundError as exc:
        st.error(str(exc))
        return

    geojson = load_geojson(GEOJSON_PATH)
    metrics = load_model_metrics(METRICS_PATH)
    model = load_catboost_model(MODEL_PATH)
    if INSPECTOR_PATH.exists():
        inspector_signature: Optional[float] = INSPECTOR_PATH.stat().st_mtime
        inspector_payload = load_inspector_payload(INSPECTOR_PATH, inspector_signature)
    else:
        inspector_signature = None
        inspector_payload = {}

    with st.sidebar:
        st.header("è¡¨ç¤ºè¨­å®š")
        tol = st.slider("æ¨ªã°ã„è¨±å®¹å¹… (ï¼…ãƒã‚¤ãƒ³ãƒˆ)", 0.0, 1.0, DEFAULT_TOLERANCE, 0.05)
        map_option = st.selectbox("åœ°å›³ã®è‰²åˆ†ã‘", MAP_OPTIONS)
        if model is None:
            st.warning(
                "CatBoostãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ã¿ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚"
            )
        if metrics:
            st.subheader("ãƒ¢ãƒ‡ãƒ«æŒ‡æ¨™")
            st.markdown('<div class="small-metric">', unsafe_allow_html=True)
            cat_metrics = metrics.get("catboost")
            if cat_metrics:
                r2 = cat_metrics.get("r2", np.nan)
                mse = cat_metrics.get("mse", np.nan)
                rmse = float(mse) ** 0.5 if pd.notna(mse) else np.nan
                st.metric("RÂ²", f"{r2:.3f}")
                st.metric("RMSE", f"{rmse:.3f}")
            st.markdown("</div>", unsafe_allow_html=True)

    cache_stale = st.session_state.get("inspector_signature") != inspector_signature

    if "model_cache" not in st.session_state or cache_stale:
        if inspector_payload:
            enriched_once = features_df.copy()
            pred_records: List[Dict[str, Any]] = []
            shap_lookup_once: Dict[str, List[str]] = {}
            tooltip_lookup_once: Dict[str, Dict[str, Any]] = {}

            for code, payload in inspector_payload.items():
                code_str = str(code).zfill(5)
                shap_lookup_once[code_str] = payload.get("shap_top3", [])
                tooltip_info = payload.get("tooltip")
                if isinstance(tooltip_info, dict):
                    tooltip_lookup_once[code_str] = tooltip_info

                record = {
                    "å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰": str(
                        payload.get("å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰", code_str)
                    ).zfill(5),
                    "baseline_pred_delta": payload.get("baseline_pred_delta"),
                    "residual_model_pred": payload.get("residual_model_pred"),
                    "pred_delta": payload.get("pred_delta"),
                    "pred_ç©ºãå®¶ç‡_2023": payload.get("pred_ç©ºãå®¶ç‡_2023"),
                    "æ®‹å·®(å®Ÿæ¸¬-äºˆæ¸¬)": payload.get("æ®‹å·®(å®Ÿæ¸¬-äºˆæ¸¬)"),
                    "Î”(23-18)": payload.get("Î”(23-18)"),
                }
                pred_records.append(record)

            if pred_records:
                pred_df = pd.DataFrame(pred_records).drop_duplicates(
                    subset=["å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰"], keep="last"
                )
                pred_df["å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰"] = (
                    pred_df["å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰"].astype(str).str.zfill(5)
                )
                pred_cols = [
                    "baseline_pred_delta",
                    "residual_model_pred",
                    "pred_delta",
                    "pred_ç©ºãå®¶ç‡_2023",
                    "â–³(å®Ÿæ¸¬-äºˆæ¸¬)",
                    "Î”(23-18)",
                ]
                for col in pred_cols:
                    if col in pred_df.columns:
                        pred_df[col] = pd.to_numeric(pred_df[col], errors="coerce")

                enriched_once = enriched_once.merge(
                    pred_df, on="å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰", how="left"
                )
            cache_messages: List[str] = []
            static_geojson = build_static_geojson(
                geojson, enriched_once, shap_lookup_once, tooltip_lookup_once
            )
        else:
            enriched_once, feature_matrix, base_messages = compute_predictions(
                features_df, model
            )
            if feature_matrix is not None and "å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰" in features_df.columns:
                code_series = features_df.loc[feature_matrix.index, "å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰"]
            else:
                code_series = None
            shap_lookup_once, shap_message_once = compute_shap_topk(
                model, feature_matrix, codes=code_series
            )
            cache_messages = base_messages.copy()
            cache_messages.append(
                "diff_model_inspector.json ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€äºˆæ¸¬ã¨SHAPã‚’å†è¨ˆç®—ã—ã¾ã—ãŸã€‚"
            )
            if shap_message_once:
                cache_messages.append(shap_message_once)
            tooltip_lookup_once = {}
            static_geojson = build_static_geojson(
                geojson, enriched_once, shap_lookup_once
            )

        st.session_state["model_cache"] = {
            "enriched_df": enriched_once,
            "shap_lookup": shap_lookup_once,
            "tooltip_lookup": tooltip_lookup_once,
            "geojson_static": static_geojson,
            "messages": cache_messages,
        }
        st.session_state["inspector_signature"] = inspector_signature

    cache = st.session_state["model_cache"]
    enriched_df = cache["enriched_df"].copy()
    shap_lookup = cache["shap_lookup"]
    tooltip_lookup = cache.get("tooltip_lookup", {})
    static_geojson = cache.get("geojson_static")
    if static_geojson is None and geojson is not None:
        static_geojson = build_static_geojson(
            geojson, enriched_df, shap_lookup, tooltip_lookup
        )
        cache["geojson_static"] = static_geojson
    info_messages = list(cache.get("messages", []))

    classified_df = classify_risk(enriched_df, tol)

    risk_lookup = (
        classified_df.set_index("å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰")["ãƒªã‚¹ã‚¯åŒºåˆ†"].to_dict()
        if "å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰" in classified_df.columns
        else {}
    )
    trend_lookup = (
        classified_df.set_index("å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰")["ãƒˆãƒ¬ãƒ³ãƒ‰"].to_dict()
        if "å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰" in classified_df.columns
        else {}
    )

    if map_option == "2023å¹´ç©ºãå®¶ç‡ï¼ˆå®Ÿæ¸¬ï¼‰":
        map_lookup = (
            classified_df.set_index("å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰")["ç©ºãå®¶ç‡_2023"].to_dict()
            if "å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰" in classified_df.columns
            else {}
        )
        legend = "2023å¹´ç©ºãå®¶ç‡ (%)"
    elif map_option == "2023å¹´ç©ºãå®¶ç‡ï¼ˆäºˆæ¸¬ï¼‰":
        map_lookup = (
            classified_df.set_index("å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰")["pred_ç©ºãå®¶ç‡_2023"].to_dict()
            if "å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰" in classified_df.columns
            else {}
        )
        legend = "äºˆæ¸¬ç©ºãå®¶ç‡ (%)"
    else:
        map_lookup = {}
        legend = ""

    update_geojson_dynamic_properties(
        static_geojson, risk_lookup, trend_lookup, map_lookup
    )
    folium_map = build_map(
        static_geojson, map_option, legend, risk_lookup, trend_lookup, map_lookup
    )

    map_col, inspector_col = st.columns((2.2, 1.0))

    with map_col:
        if folium_map is None:
            st.warning(
                "Folium ã¾ãŸã¯ GeoJSON ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€åœ°å›³ã¯è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚"
            )
            map_state = {}
        else:
            map_state = st_folium(
                folium_map,
                height=650,
                use_container_width=True,
                key="akiya_map",
            )

    selected_code = st.session_state.get("selected_code")
    props = None
    if map_state:
        drawing = map_state.get("last_active_drawing") or map_state.get(
            "last_object_clicked"
        )
        if drawing and isinstance(drawing, dict):
            props = drawing.get("properties", {})
    if props:
        code = props.get("akiya_code") or props.get("å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰")
        if code:
            selected_code = str(code).zfill(5)
            st.session_state["selected_code"] = selected_code

    with inspector_col:
        render_inspector(
            selected_code, classified_df, shap_lookup, risk_lookup, trend_lookup
        )

    st.subheader("è‡ªæ²»ä½“åˆ¥ãƒ†ãƒ¼ãƒ–ãƒ«")
    residual_col = next(
        (
            col
            for col in ("â–³(å®Ÿæ¸¬-äºˆæ¸¬)", "æ®‹å·®(å®Ÿæ¸¬-äºˆæ¸¬)")
            if col in classified_df.columns
        ),
        None,
    )
    table_cols = [
        "å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰",
        "å¸‚åŒºç”ºæ‘å",
        "éƒ½é“åºœçœŒå",
        "ç©ºãå®¶ç‡_2018",
        "ç©ºãå®¶ç‡_2023",
        "Î”(23-18)",
        "ãƒªã‚¹ã‚¯åŒºåˆ†",
        "ãƒˆãƒ¬ãƒ³ãƒ‰",
        "pred_ç©ºãå®¶ç‡_2023",
    ]
    if residual_col:
        table_cols.append(residual_col)
    existing_cols = [col for col in table_cols if col in classified_df.columns]
    table_data = classified_df[existing_cols].copy()
    table_data = table_data.rename(
        columns={
            "pred_ç©ºãå®¶ç‡_2023": "ç©ºãå®¶ç‡(äºˆæ¸¬)",
            "æ®‹å·®(å®Ÿæ¸¬-äºˆæ¸¬)": "â–³(å®Ÿæ¸¬-äºˆæ¸¬)",
        }
    )
    if "ãƒªã‚¹ã‚¯åŒºåˆ†" in table_data.columns:
        table_data = table_data.sort_values("ãƒªã‚¹ã‚¯åŒºåˆ†")
    st.dataframe(table_data, use_container_width=True)

    if info_messages:
        st.subheader("ãƒ­ã‚° / æ³¨æ„äº‹é …")
        for msg in info_messages:
            if not msg:
                continue
            st.markdown(f"- {msg}")

    st.caption("ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: data/processed/features_master__wide__v1.csv ä»–")


# ---------------------------------------------------------------------------
# Groqãƒ˜ãƒ«ãƒ‘
# ---------------------------------------------------------------------------
def _active_signature() -> str:
    """å†å­¦ç¿’ã§å¤‰ã‚ã‚‹ç½²åã€‚ãªã‘ã‚Œã°ãƒ¢ãƒ‡ãƒ«mtimeã‚’ä»£ç”¨ã€‚"""
    import streamlit as st

    sig = st.session_state.get("inspector_signature")
    if sig is None and MODEL_PATH.exists():
        sig = MODEL_PATH.stat().st_mtime
    return str(sig or "0")


def _ai_cache_path(signature: str) -> Path:
    AI_CACHE_DIR.mkdir(parents=True, exist_ok=True)
    return AI_CACHE_DIR / f"groq_cache__{signature}.json"


@st.cache_resource(show_spinner=False)
def _load_ai_cache(signature: str) -> dict:
    p = _ai_cache_path(signature)
    if p.exists():
        with p.open("r", encoding="utf-8") as f:
            return json.load(f)
    return {}


def _save_ai_cache(signature: str, data: dict) -> None:
    p = _ai_cache_path(signature)
    tmp = p.with_suffix(".tmp")
    with tmp.open("w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    tmp.replace(p)


def _ai_cache_key(model: str, payload: dict) -> str:
    # æ—¢å­˜ã® _fingerprint_payload ã‚’å†åˆ©ç”¨
    return f"{model}:{_fingerprint_payload(payload)}"


def _get_groq_client() -> Optional["Groq"]:
    """st.secrets ã¾ãŸã¯ ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ API Key ã‚’å–å¾—ã—ã¦ Groq ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’è¿”ã™ã€‚"""
    if Groq is None:
        return None
    api_key = None
    try:
        import streamlit as st

        api_key = st.secrets.get("GROQ_API_KEY", None)
    except Exception:
        pass
    api_key = api_key or os.environ.get("GROQ_API_KEY")
    if not api_key:
        return None
    try:
        return Groq(api_key=api_key)
    except Exception:
        return None


def _fingerprint_payload(payload: dict) -> str:
    """å†…å®¹ãŒåŒã˜ãªã‚‰å†åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ä½œã‚‹ã€‚"""
    blob = json.dumps(payload, ensure_ascii=False, sort_keys=True)
    return hashlib.sha256(blob.encode("utf-8")).hexdigest()


@st.cache_data(show_spinner=False, ttl=60 * 60 * 24)
# --- æ—¢å­˜ã® generate_ai_suggestion_with_groq ã‚’ç½®ãæ›ãˆ ---


@st.cache_data(show_spinner=False, ttl=60 * 60 * 24)
def generate_ai_suggestion_with_groq(
    payload: dict, model: str = "llama-3.1-8b-instant"
) -> dict:
    """
    Groqã«ã€Œè¶…ç°¡æ½”ãƒ†ãƒ³ãƒ—ãƒ¬ã€ã§å•ã„åˆã‚ã›ã€JSONã‚’å—ã‘å–ã‚ŠMarkdownåŒ–ã—ã¦è¿”ã™ã€‚
    è¿”ã‚Šå€¤: {'ok': bool, 'text': str, 'raw': dict, 'usage': dict|None}
    """
    client = _get_groq_client()
    if client is None:
        return {
            "ok": False,
            "text": "ğŸ”‘ GROQ_API_KEY ãŒæœªè¨­å®šã®ãŸã‚å®Ÿè¡Œã§ãã¾ã›ã‚“ã€‚",
            "raw": {},
            "usage": None,
        }

    # --- ãƒ†ãƒ³ãƒ—ãƒ¬ï¼ˆLLMã«ã¯â€œã“ã‚Œã ã‘â€ã‚’åŸ‹ã‚ã¦ã‚‚ã‚‰ã†ï¼‰ ---
    system = (
        "ã‚ãªãŸã¯æ—¥æœ¬ã®ç©ºãå®¶å¯¾ç­–ã«è©³ã—ã„DXã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã€‚"
        "å‡ºåŠ›ã¯ JSON ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿ã€‚å¿…ãšæ¬¡ã®ã‚¹ã‚­ãƒ¼ãƒã«å¾“ã†ï¼š"
        '{"analysis":["..."],"ideas":["..."]}'
        "åˆ¶ç´„ï¼š"
        "- æ–‡ä½“ã¯çŸ­æ–‡ãƒ»ç®‡æ¡æ›¸ãã€‚å„æ–‡ã¯50å­—ä»¥å†…ã€‚"
        "- æ•°å€¤ã¯æ–°ãŸã«ä½œã‚‰ãªã„ï¼ˆå…¥åŠ›å€¤ã®ã¿è¨€åŠå¯ï¼‰ã€‚"
        "- åˆ†æã¯æœ€å¤§2ç‚¹ã€ææ¡ˆã¯æœ€å¤§3ç‚¹ã€‚é‡è¤‡ã‚„æŠ½è±¡è¡¨ç¾ã¯é¿ã‘ã‚‹ã€‚"
        "- å›ºæœ‰åè©ã¯å¿…è¦æœ€å°é™ã€‚ä¸€èˆ¬åè©ã§å…·ä½“ç­–ã‚’ç¤ºã™ã€‚"
        "- å‡ºåŠ›ã«èª¬æ˜æ–‡ã‚„æ¥é ­è¾ãƒ»æ¥å°¾è¾ã‚’ä»˜ã‘ãªã„ï¼ˆJSONã®ã¿ï¼‰ã€‚"
    )

    user = f"""
    # å…¥åŠ›
    å¸‚åŒºç”ºæ‘: {payload.get('pref_name')} {payload.get('city_name')}ï¼ˆ{payload.get('city_code')}ï¼‰
    ç©ºãå®¶ç‡2018: {payload.get('vac18')}
    ç©ºãå®¶ç‡2023: {payload.get('vac23')}
    å¤‰åŒ–pt(23-18): {payload.get('delta')}
    ãƒªã‚¹ã‚¯åŒºåˆ†: {payload.get('risk_label')}
    ãƒˆãƒ¬ãƒ³ãƒ‰: {payload.get('trend')}
    SHAPè¦å› Top3:
    - {payload.get('factor1')}
    - {payload.get('factor2')}
    - {payload.get('factor3')}

    # å‡ºåŠ›å½¢å¼ï¼ˆå³å®ˆï¼‰
    {{
      "analysis": ["çŸ­æ–‡1","çŸ­æ–‡2"],     // å¸‚æ³ã®å«æ„ã‚„è§£é‡ˆï¼ˆæœ€å¤§2ï¼‰
      "ideas":    ["æ–½ç­–1","æ–½ç­–2","æ–½ç­–3"] // å®Ÿè¡Œå¯èƒ½ãªå¯¾ç­–ï¼ˆæœ€å¤§3ï¼‰
    }}
    """

    try:
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system},
                {"role": "user", "content": user},
            ],
            temperature=0.2,
            response_format={"type": "json_object"},
            max_tokens=400,
        )
        content = resp.choices[0].message.content if resp.choices else ""
        usage = getattr(resp, "usage", None)

        data = json.loads(content) if content else {"analysis": [], "ideas": []}

        # Markdown æ•´å½¢ï¼ˆå…±é€šæ³¨æ„æ–‡ã¯ã“ã“ã§ã¯å…¥ã‚Œãªã„ï¼‰
        blocks = []

        if data.get("analysis"):
            block = ["**è¦å› åˆ†æ**"]
            block += [f"- {x}" for x in data["analysis"][:2]]
            blocks.append("\n".join(block))

        if data.get("ideas"):
            block = ["**åˆ©æ´»ç”¨ææ¡ˆ**"]
            block += [f"- {x}" for x in data["ideas"][:3]]
            blocks.append("\n".join(block))

        # â† ãƒã‚¤ãƒ³ãƒˆï¼šã‚»ã‚¯ã‚·ãƒ§ãƒ³é–“ã¯ \n\n ã§çµåˆã—ã¦1è¡Œã®ç©ºç™½è¡Œã‚’å…¥ã‚Œã‚‹
        text = "\n\n".join(blocks) if blocks else "ï¼ˆå‡ºåŠ›ãªã—ï¼‰"

        return {"ok": True, "text": text, "raw": data, "usage": usage}
    except Exception as e:
        return {"ok": False, "text": f"APIã‚¨ãƒ©ãƒ¼: {e}", "raw": {}, "usage": None}


def render_inspector(
    selected_code: Optional[str],
    df: pd.DataFrame,
    shap_lookup: dict[str, list[str]],
    risk_lookup: dict[str, str],
    trend_lookup: dict[str, str],
) -> None:
    st.subheader("è‡ªæ²»ä½“ã‚¤ãƒ³ã‚¹ãƒšã‚¯ã‚¿")
    if not selected_code:
        st.info("åœ°å›³ä¸Šã®è‡ªæ²»ä½“ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨è©³ç´°ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
        return

    row = df[df["å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰"].astype(str).str.zfill(5) == selected_code]
    if row.empty:
        st.warning("é¸æŠã—ãŸè‡ªæ²»ä½“ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    rec = row.iloc[0]

    # å°ã•ãªãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿
    def fmt_pct(x):
        return "â€“" if pd.isna(x) else f"{float(x):.2f}%"

    def fmt_delta(x):
        return "" if pd.isna(x) else f"{float(x):+.2f} pt"

    vac18 = rec.get("ç©ºãå®¶ç‡_2018", np.nan)
    vac23 = rec.get("ç©ºãå®¶ç‡_2023", np.nan)
    delta = rec.get("Î”(23-18)", np.nan)
    pred = rec.get("pred_ç©ºãå®¶ç‡_2023", np.nan)
    resid = rec.get("â–³(å®Ÿæ¸¬-äºˆæ¸¬", rec.get("æ®‹å·®(å®Ÿæ¸¬-äºˆæ¸¬)", np.nan))

    # â˜… HTMLã®<div>ãƒ©ãƒƒãƒ‘ã¯ä½¿ã‚ãšã€å…¬å¼ã®ã‚³ãƒ³ãƒ†ãƒŠã«å¤‰æ›´ï¼ˆä½™è¨ˆãªæ ãŒå‡ºãªã„ï¼‰
    with st.container(border=True):
        st.markdown(
            f'<div class="akiya-title">ğŸ˜ï¸ {rec.get("å¸‚åŒºç”ºæ‘å","ä¸æ˜")} '
            f'<span class="akiya-subtle">({selected_code})</span></div>',
            unsafe_allow_html=True,
        )
        st.markdown(
            '<div class="akiya-badges">'
            + risk_badge(risk_lookup.get(selected_code, DEFAULT_RISK_LABEL))
            + f'<span class="akiya-pill">{trend_lookup.get(selected_code,"")}</span>'
            + "</div>",
            unsafe_allow_html=True,
        )

        # å€¤ãŒåˆ‡ã‚Œãªã„ã‚ˆã†ã« 2018 ã¨ 2023 ã‚’åˆ†ã‘ã¦è¡¨ç¤º
        c1, c2 = st.columns(2)
        with c1:
            st.metric("ç©ºãå®¶ç‡ 2018", fmt_pct(vac18))
        with c2:
            st.metric("ç©ºãå®¶ç‡ 2023", fmt_pct(vac23), delta=fmt_delta(delta))

        # äºˆæ¸¬/æ®‹å·®ã¯â€œå‚è€ƒè¡¨ç¤ºâ€ã¨ã—ã¦æ§ãˆã‚ã«
        st.markdown(
            f'<div class="akiya-muted">äºˆæ¸¬(2023): {fmt_pct(pred)} / æ®‹å·®: {fmt_delta(resid)}</div>',
            unsafe_allow_html=True,
        )

        tab1, tab2, tab3 = st.tabs(["è¦å›  Top3", "ç”Ÿãƒ‡ãƒ¼ã‚¿", "AIææ¡ˆ(Groq)"])
        with tab1:
            top_factors = shap_lookup.get(selected_code) or []
            if top_factors:
                for item in top_factors:
                    st.markdown(f"- {item}")
            else:
                st.caption("SHAPæƒ…å ±ãªã—")

        with tab2:
            # åˆ—åã‚†ã‚‰ãã‚’å¸å
            residual_show_col = next(
                (
                    col
                    for col in ("â–³(å®Ÿæ¸¬-äºˆæ¸¬)", "æ®‹å·®(å®Ÿæ¸¬-äºˆæ¸¬)")
                    if col in df.columns
                ),
                None,
            )
            show_cols = [
                "éƒ½é“åºœçœŒå",
                "å¸‚åŒºç”ºæ‘å",
                "ç©ºãå®¶ç‡_2018",
                "ç©ºãå®¶ç‡_2023",
                "Î”(23-18)",
                "pred_ç©ºãå®¶ç‡_2023",
            ]
            if residual_show_col:
                show_cols.append(residual_show_col)
            show_cols = [c for c in show_cols if c in df.columns]

            detail_df = row[show_cols].rename(
                columns={
                    "pred_ç©ºãå®¶ç‡_2023": "ç©ºãå®¶ç‡(äºˆæ¸¬)",
                    "æ®‹å·®(å®Ÿæ¸¬-äºˆæ¸¬)": "â–³(å®Ÿæ¸¬-äºˆæ¸¬)",
                }
            )
            st.dataframe(detail_df, use_container_width=True, hide_index=True)

        with tab3:
            # ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆç°¡æ˜“ï¼‰
            model = st.selectbox(
                "ãƒ¢ãƒ‡ãƒ«",
                options=["llama-3.1-8b-instant", "llama-3.3-70b-versatile"],
                index=0,
                help="é€Ÿåº¦é‡è¦–: 8B / å“è³ªé‡è¦–: 70B",  # å®Ÿé‹ç”¨ã¯8Bæ¨å¥¨
            )

            # ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ä½œæˆ
            factors = (shap_lookup.get(selected_code) or []) + ["", "", ""]
            payload = {
                "city_code": selected_code,
                "pref_name": rec.get("éƒ½é“åºœçœŒå", ""),
                "city_name": rec.get("å¸‚åŒºç”ºæ‘å", ""),
                "vac18": rec.get("ç©ºãå®¶ç‡_2018", ""),
                "vac23": rec.get("ç©ºãå®¶ç‡_2023", ""),
                "delta": rec.get("Î”(23-18)", ""),
                "risk_label": risk_lookup.get(selected_code, ""),
                "trend": trend_lookup.get(selected_code, ""),
                "factor1": factors[0],
                "factor2": factors[1],
                "factor3": factors[2],
            }
            sig = _active_signature()
            cache_db = _load_ai_cache(sig)
            ckey = _ai_cache_key(model, payload)

            # å®Ÿè¡ŒUI
            colA, colB, colC = st.columns([1, 1, 2])
            with colA:
                go = st.button("ğŸ’¡ ææ¡ˆã‚’ç”Ÿæˆ", type="primary")
            with colB:
                force_refresh = st.button("ğŸ”„ å†ç”Ÿæˆ")
                # st.caption(
                #     "â€» é€ã‚‹ã®ã¯ã€è‡ªæ²»ä½“å/ã‚³ãƒ¼ãƒ‰ãƒ»ç©ºãå®¶ç‡ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»SHAPä¸Šä½3ã¤ã®èª¬æ˜ã€‘ã®ã¿ã€‚æ©Ÿå¯†ãƒ‡ãƒ¼ã‚¿ã¯é€ä¿¡ã—ã¾ã›ã‚“ã€‚"
                # )
            with colC:
                use_cache = st.toggle(
                    "ã‚­ãƒ£ãƒƒã‚·ãƒ¥å„ªå…ˆ",
                    value=True,
                    help="å†å­¦ç¿’(ç½²åå¤‰æ›´)ã¾ã§ã¯å•ã„åˆã‚ã›ãšå†åˆ©ç”¨ã—ã¾ã™",
                )

            # å®Ÿè¡Œ
            if go:
                if use_cache and not force_refresh and ckey in cache_db:
                    st.markdown(cache_db[ckey]["text"])
                    st.caption(f"ğŸ§  cached â€¢ {cache_db[ckey].get('ts','')}")
                else:
                    with st.spinner("Groqã«å•ã„åˆã‚ã›ä¸­â€¦"):
                        result = generate_ai_suggestion_with_groq(payload, model=model)
                    if result["ok"]:
                        st.markdown(result["text"])
                        if result.get("usage"):
                            u = result["usage"]
                            st.caption(f"tokens: {getattr(u, 'total_tokens', 'â€”')}")
                    else:
                        st.warning(result["text"])

            # å›ºå®šã®å…±é€šæ³¨æ„æ–‡ï¼ˆLLMå‡ºåŠ›ã¨ç‹¬ç«‹ã—ã¦å¸¸æ™‚è¡¨ç¤ºï¼‰
            st.markdown("**å…±é€šã®å‰æãƒ»æ³¨æ„ï¼ˆå›ºå®šï¼‰**")
            for note in COMMON_FIXED_NOTES:
                st.markdown(f"- {note}")

            # APIã‚­ãƒ¼æœªè¨­å®šã®ã¨ãã®ãƒ’ãƒ³ãƒˆ
            if _get_groq_client() is None:
                st.info(
                    "å®Ÿè¡Œã«ã¯ `GROQ_API_KEY` ã®è¨­å®šãŒå¿…è¦ã§ã™ã€‚Streamlit Cloudã¯ `st.secrets`ã€ãƒ­ãƒ¼ã‚«ãƒ«ã¯ç’°å¢ƒå¤‰æ•°ã§è¨­å®šã—ã¦ãã ã•ã„ã€‚"
                )


if __name__ == "__main__":
    main()
